---
title: "Assignment-5 Q&A Bilge Salman (03796071), Salim Kaplan (0378856) GitHub link: https://github.com/salimkaplan/BayesIntro24-Assignments/tree/main"
format: pdf
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: sentence
---

```{r}
#| echo: true
#| eval: true
#| output: false
# load packages here
library(rethinking)
library(tidyverse)
library(tidyr)

knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(width.cutoff = 50))
```

Load the data set RiskyChoice.csv to solve the Task Set 1. Use the read_csv2() function
instead of read_csv().
```{r}
#| echo: true
#| eval: true
#| output: false
# Load the data set
data <- read_csv2("RiskyChoice.csv")
```

## Task Set 1

### 1.1
Create a reduced data table with only one row per subject that shows the number of solved
choices problems (nChoice) and the number of correct choices (nCorrect) for each subject
along with the other variables. Remove the subjects with missing values. Print the data of
the first 10 subjects.
```{r}
# 1.1
# Summarize data
reduced_data <- data %>%
  group_by(Subject) %>%
  summarise(
    nChoice = sum(CorrectChoice, na.rm = TRUE) + sum(RiskyChoice, na.rm = TRUE),
    nCorrect = sum(CorrectChoice, na.rm = TRUE),
    Numeracy = first(Numeracy),  
    AgeGroup = first(AgeGroup),  
    Gender = first(Gender)       
  ) %>%
  na.omit() 

print(head(reduced_data, 10))
```

### 1.2
Run a Bayesian regression model that predicts nCorrect from Numeracy using fixed intercepts
and fixed slopes. Standardize the predictor before running the model and compute the WAIC
of the model.
```{r}
# 1.2
reduced_data$Numeracy_s <- scale(reduced_data$Numeracy)

data_list <- list(
  nCorrect = reduced_data$nCorrect,
  Numeracy = reduced_data$Numeracy_s
)
```


```{r}
#| echo: true
#| eval: true
#| output: false
# Bayesian regression model
model_fixed <- ulam(
  alist(
    nCorrect ~ dnorm(mu, sigma),
    mu <- a + b * Numeracy,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), 
  data = data_list,
  chains = 4,
  cores = 4,
  log_lik = TRUE)

```

```{r}
# WAIC
waic_fixed <- WAIC(model_fixed)
print(waic_fixed)
```

### 1.3
Run a Bayesian regression model that predicts nCorrect from Numeracy using random intercepts
and fixed slopes. Standardize the predictor before running the model and compute the
WAIC of the model.
```{r}
# 1.3
# list for ulam with subject indices
data_list_random <- list(
  nCorrect = reduced_data$nCorrect,
  Numeracy = reduced_data$Numeracy_s,
  Subject = as.integer(as.factor(reduced_data$Subject)))
```

```{r}
#| echo: true
#| eval: true
#| output: false
# Bayesian regression model 
model_random <- ulam(
  alist(
    nCorrect ~ dnorm(mu, sigma),
    mu <- a[Subject] + b * Numeracy,
    a[Subject] ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)), 
  data = data_list_random,
  chains = 4,
  cores = 4,
  log_lik = TRUE)
```


```{r}
# WAIC
waic_random <- WAIC(model_random)
print(waic_random)

compare_models <- compare(model_fixed, model_random)
print(compare_models)
```

## Task Set 2
### 2.1

Create a data table that entails 10,000 posterior samples (rows) for each subject-specific
(columns) intercept. Convert the sampled values into probabilities and print the first 10
samples of the first 10 subjects.
```{r}
# posterior samples for subject-specific intercepts
posterior_samples <- extract.samples(model_random)

subject_intercepts <- posterior_samples$a

# converting the sampled values into probabilities using the logistic function
subject_probabilities <- 1 / (1 + exp(-subject_intercepts))

posterior_table <- as.data.frame(subject_probabilities)

print(head(posterior_table[, 1:10], 10))
```


## 2.2
Use the posterior samples to plot the posterior distribution of all subject-specific intercepts to
show the variability in the performance among subjects. Use the converted values (probabilities).
```{r}
# pivot data to a longer format
posterior_long <- pivot_longer(posterior_table, cols = everything(), names_to = "Subject", values_to = "Probability")

# posterior density plot
ggplot(posterior_long, aes(x = Probability, group = Subject)) +
  geom_density(alpha = 0.5) +
  theme_light()
```

## 2.3

Consider the following posterior summaries and trace plots. Which model was estimated and what might be the cause of the convergence problems?

- For the estimated model, the parameters mu_a and tau_a are representing the mean and standard deviation of the intercepts.

- The same applies for the parameters mu_b and tau_b for the slopes, suggesting a  multilevel model with random intercepts and slopes.

- tau_b is showing signs of convergence issues, which may indicate that the model is not well-specified or that the data.

- Having a low value ess_bulk of 43.68 and an rhat of 1.11 which should have been close to 1 may indicate potential convergence issues.


